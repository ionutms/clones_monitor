name: Monitor Repository Clones and Update Traffic Data

on:
  schedule:
    - cron: '0 */2 * * *'
  workflow_dispatch:
    inputs:
      days_to_collect:
        description: 'Number of days to collect (1-14, default: 2)'
        required: false
        default: '2'
        type: string
      force_full_sync:
        description: 'Force full 14-day sync (ignores days_to_collect)'
        required: false
        default: false
        type: boolean
  push:
    branches: [main]

# Centralized repository configuration - ADD NEW PROJECTS HERE ONLY
env:
  # Repository mapping: prefix=Repository_Name (separated by spaces)
  REPO_CONFIG: |
    kicad_symbols_generator=KiCAD_Symbols_Generator
    minimal_adp1032=Minimal_ADP1032
    minimal_max14906=Minimal_MAX14906
    minimal_ad74413r=Minimal_AD74413R
    modular_software_configurable_io_plc=Modular_Software_Configurable_IO_PLC
    minimal_adin1110=Minimal_ADIN1110
    minimal_ltc9111=Minimal_LTC9111
    minimal_max17761=Minimal_MAX17761
    minimal_lt8304=Minimal_LT8304
    minimal_ltc3350=Minimal_LTC3350
    supercap_bank=Supercap_Bank
    minimal_ad74416h=Minimal_AD74416H
    minimal_ad74416h_stack_adapter=Minimal_AD74416H_Stack_Adapter
    ad74416h_power_interface_module=AD74416H_Power_Interface_Module
    modular_ad74416h_plc=Modular_AD74416H_PLC
    minimal_adp1031=Minimal_ADP1031
    minimal_adp1074=Minimal_ADP1074
    minimal_adpl76030=Minimal_ADPL76030
    minimal_max17570=Minimal_MAX17570
    3d_models_vault=3D_Models_Vault
    minimal_max32650=Minimal_MAX32650
    docker_kicad_learning=docker_kicad_learning
    docker_3d_models_hosting=Docker_3D_Models_Hosting
    clones_monitor=clones_monitor
    workflow_distributor=workflow_distributor
    systemverilog_learning=SystemVerilog_Learning
    uvm_learning=uvm_learning
    langgraph_learning=LangGraph_Learning
    model_context_protocol_learning=Model_Context_Protocol_Learning
    pdf_rag_learning=PDF_RAG_Learning
  # Default number of days to collect (can be overridden by workflow_dispatch)
  DEFAULT_DAYS_TO_COLLECT: '2'

jobs:
  monitor-traffic:
    runs-on: ubuntu-latest
    env:
      TZ: Europe/Bucharest
      GH_TOKEN: ${{ secrets.TRAFIC_UPDATE_TOKEN }}
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Verify GitHub CLI
      run: gh --version

    - name: Set collection parameters
      run: |
        # Determine how many days to collect
        if [ "${{ github.event.inputs.force_full_sync }}" = "true" ]; then
          DAYS_TO_COLLECT=14
          echo "COLLECTION_MODE=full_sync" >> $GITHUB_ENV
        elif [ -n "${{ github.event.inputs.days_to_collect }}" ]; then
          DAYS_TO_COLLECT="${{ github.event.inputs.days_to_collect }}"
          echo "COLLECTION_MODE=manual" >> $GITHUB_ENV
        else
          DAYS_TO_COLLECT="${{ env.DEFAULT_DAYS_TO_COLLECT }}"
          echo "COLLECTION_MODE=scheduled" >> $GITHUB_ENV
        fi
        
        # Validate days range (GitHub API provides max 14 days)
        if [ "$DAYS_TO_COLLECT" -gt 14 ]; then
          echo "Warning: GitHub API only provides 14 days of data. Setting to 14."
          DAYS_TO_COLLECT=14
        elif [ "$DAYS_TO_COLLECT" -lt 1 ]; then
          echo "Warning: Days to collect must be at least 1. Setting to 1."
          DAYS_TO_COLLECT=1
        fi
        
        echo "DAYS_TO_COLLECT=$DAYS_TO_COLLECT" >> $GITHUB_ENV
        echo "Will collect data for the last $DAYS_TO_COLLECT days (mode: $COLLECTION_MODE)"

    - name: Initialize CSV files
      run: |
        # Parse centralized configuration
        declare -A files=()
        while IFS='=' read -r prefix repo_name; do
          if [[ -n "$prefix" && -n "$repo_name" ]]; then
            files["$prefix"]="$repo_name"
          fi
        done <<< "$REPO_CONFIG"
        
        for file_prefix in "${!files[@]}"; do
          # Initialize clones file
          if [ ! -f "${file_prefix}_clones_history.csv" ]; then
            echo "clone_timestamp,total_clones,unique_clones" > "${file_prefix}_clones_history.csv"
          fi
          
          # Initialize visitors file
          if [ ! -f "${file_prefix}_visitors_history.csv" ]; then
            echo "visitor_timestamp,total_visitors,unique_visitors" > "${file_prefix}_visitors_history.csv"
          fi
        done

    - name: Fetch traffic data for multiple days
      run: |
        fetch_traffic_data() {
          local repo=$1
          local output_prefix=$2
          local type=$3
          local endpoint="clones"
          local output_type="clones"
          
          if [ "$type" = "visitors" ]; then
            endpoint="views"
            output_type="visitors"
          fi
          
          echo "Processing $type data for $repo (${output_prefix}) - Last $DAYS_TO_COLLECT days..."
          
          # Get data from the GitHub API (includes up to 14 days)
          traffic_data=$(gh api "repos/ionutms/$repo/traffic/$endpoint?per_page=100&ref=main")
          
          # Check if we have any data
          if [ -z "$traffic_data" ]; then
            echo "  No data available from GitHub API"
            return
          fi
          
          # Generate list of dates to process
          dates_to_process=()
          for ((i=0; i<DAYS_TO_COLLECT; i++)); do
            date_str=$(date -d "$i days ago" +"%Y-%m-%d")
            dates_to_process+=("$date_str")
          done
          
          echo "  Dates to process: ${dates_to_process[*]}"
          
          # Process data for each target date
          for target_date in "${dates_to_process[@]}"; do
            echo "  Processing data for $target_date..."
            
            # Check if we have existing data for the target date
            local old_data=""
            if grep -q "^$target_date" "${output_prefix}_${output_type}_history.csv"; then
              old_data=$(grep "^$target_date" "${output_prefix}_${output_type}_history.csv")
              echo "    Found existing data for $target_date: $old_data"
            else
              echo "    No existing data found for $target_date"
            fi
            
            # Extract data for the target date from API response
            day_data=$(echo "$traffic_data" | \
            jq -r "
              .${endpoint} |
              map(select(.timestamp | startswith(\"$target_date\"))) |
              map([
                (.timestamp | sub(\"T.*\";\"\") | sub(\"Z\$\";\"\")) ,
                .count,
                .uniques
              ] | @csv)[]
            ")
            
            # If we have data for the target date, process it
            if [ ! -z "$day_data" ]; then
              echo "    New data for $target_date: $day_data"
              
              # Calculate differences if we had old data
              if [ ! -z "$old_data" ]; then
                local old_total=$(echo "$old_data" | cut -d',' -f2)
                local old_unique=$(echo "$old_data" | cut -d',' -f3)
                local new_total=$(echo "$day_data" | cut -d',' -f2)
                local new_unique=$(echo "$day_data" | cut -d',' -f3)
                
                # Determine update strategy based on collection mode
                local should_update=false
                if [ "$COLLECTION_MODE" = "full_sync" ]; then
                  # In full sync mode, always update with API data
                  should_update=true
                  echo "    Full sync mode: updating with API data"
                elif (( new_total >= old_total && new_unique >= old_unique )); then
                  # In normal mode, only update if new values are higher
                  should_update=true
                  local diff_total=$((new_total - old_total))
                  local diff_unique=$((new_unique - old_unique))
                  echo "    Changes to update:"
                  echo "      Total $output_type: $old_total → $new_total ($([ $diff_total -ge 0 ] && echo "+$diff_total" || echo "$diff_total"))"
                  echo "      Unique $output_type: $old_unique → $new_unique ($([ $diff_unique -ge 0 ] && echo "+$diff_unique" || echo "$diff_unique"))"
                else
                  echo "    Keeping existing data as it has higher values"
                  day_data="$old_data"
                fi
              else
                echo "    First data for $target_date: $(echo $day_data | tr ',' ' ')"
              fi
              
              # Update the CSV file
              # First create a temporary file with current data excluding target date entries
              grep -v "^$target_date" "${output_prefix}_${output_type}_history.csv" > "${output_prefix}_${output_type}_temp.csv" || touch "${output_prefix}_${output_type}_temp.csv"
              
              # Add header if the temp file is empty
              if [ ! -s "${output_prefix}_${output_type}_temp.csv" ]; then
                if [ "$type" = "visitors" ]; then
                  echo "visitor_timestamp,total_visitors,unique_visitors" > "${output_prefix}_${output_type}_temp.csv"
                else
                  echo "clone_timestamp,total_clones,unique_clones" > "${output_prefix}_${output_type}_temp.csv"
                fi
              fi
              
              # Append target date data
              echo "$day_data" >> "${output_prefix}_${output_type}_temp.csv"
              
              # Replace original file with updated one
              mv "${output_prefix}_${output_type}_temp.csv" "${output_prefix}_${output_type}_history.csv"
              echo "    Updated ${output_prefix}_${output_type}_history.csv with $target_date data"
            else
              echo "    No data available for $target_date from GitHub API"
            fi
          done
          echo ""
        }
        
        # Parse centralized configuration
        declare -A repos=()
        while IFS='=' read -r prefix repo_name; do
          if [[ -n "$prefix" && -n "$repo_name" ]]; then
            repos["$prefix"]="$repo_name"
          fi
        done <<< "$REPO_CONFIG"
        
        for output_prefix in "${!repos[@]}"; do
          fetch_traffic_data "${repos[$output_prefix]}" "$output_prefix" "clones"
          fetch_traffic_data "${repos[$output_prefix]}" "$output_prefix" "visitors"
        done

    - name: Remove duplicate rows and sort by date
      run: |
        process_csv() {
          local input=$1
          local header=$2
          local temp="${input%.csv}_fixed.csv"
          
          echo "$header" > "$temp"
          # Remove duplicates by date and sort chronologically (oldest first - matches original behavior)
          awk -F, 'NR>1 { data[$1] = $0 } END { for(k in data) print data[k] }' "$input" | \
            sort -t, -k1 >> "$temp"
          mv "$temp" "$input"
        }
        
        for csv in *_clones_history.csv; do
          process_csv "$csv" "clone_timestamp,total_clones,unique_clones"
        done
        
        for csv in *_visitors_history.csv; do
          process_csv "$csv" "visitor_timestamp,total_visitors,unique_visitors"
        done

    - name: Generate collection summary
      run: |
        echo "=== TRAFFIC DATA COLLECTION SUMMARY ==="
        echo "Collection mode: $COLLECTION_MODE"
        echo "Days collected: $DAYS_TO_COLLECT"
        echo "Timestamp: $(date +"%Y-%m-%d %H:%M:%S %Z")"
        echo ""
        
        # Generate summary for each repository
        declare -A repos=()
        while IFS='=' read -r prefix repo_name; do
          if [[ -n "$prefix" && -n "$repo_name" ]]; then
            repos["$prefix"]="$repo_name"
          fi
        done <<< "$REPO_CONFIG"
        
        for output_prefix in "${!repos[@]}"; do
          repo_name="${repos[$output_prefix]}"
          echo "Repository: $repo_name"
          
          # Show clones data (last few entries since data is sorted oldest first)
          if [ -f "${output_prefix}_clones_history.csv" ]; then
            echo "  Recent Clones:"
            tail -n $DAYS_TO_COLLECT "${output_prefix}_clones_history.csv" | \
            while IFS=',' read -r date total unique; do
              echo "    $date: $total total ($unique unique)"
            done
          fi
          
          # Show visitors data (last few entries since data is sorted oldest first)
          if [ -f "${output_prefix}_visitors_history.csv" ]; then
            echo "  Recent Visitors:"
            tail -n $DAYS_TO_COLLECT "${output_prefix}_visitors_history.csv" | \
            while IFS=',' read -r date total unique; do
              echo "    $date: $total views ($unique unique visitors)"
            done
          fi
          echo ""
        done

    - name: Commit and push traffic data
      run: |
        git config user.name 'GitHub Actions Bot'
        git config user.email '<>'
        git add *_history.csv
        
        if [[ -n $(git status -s) ]]; then
          echo "Changes detected in traffic data files:"
          git diff --staged --stat
          
          # Generate commit message based on collection mode
          if [ "$COLLECTION_MODE" = "full_sync" ]; then
            commit_msg="Full sync: Update traffic statistics for last 14 days ($(date +"%Y-%m-%d %H:%M:%S %Z"))"
          elif [ "$COLLECTION_MODE" = "manual" ]; then
            commit_msg="Manual collection: Update traffic statistics for last $DAYS_TO_COLLECT days ($(date +"%Y-%m-%d %H:%M:%S %Z"))"
          else
            commit_msg="Scheduled update: Traffic statistics for last $DAYS_TO_COLLECT days ($(date +"%Y-%m-%d %H:%M:%S %Z"))"
          fi
          
          git commit -m "$commit_msg"
          git pull --rebase origin main
          git push origin main
          echo "Successfully pushed updated traffic data"
        else
          echo "No changes detected in traffic data files"
        fi

  update-repo-traffic:
    runs-on: ubuntu-latest
    needs: monitor-traffic
    
    steps:
    - uses: actions/checkout@v4
      with:
        repository: ionutms/clones_monitor
        path: clones_monitor
        token: ${{ secrets.TRAFIC_UPDATE_TOKEN }}
    
    - uses: actions/checkout@v4
      with:
        repository: ionutms/KiCAD_Symbols_Generator
        path: repo1
        token: ${{ secrets.TRAFIC_UPDATE_TOKEN }}
    
    - name: Copy and update traffic data
      run: |
        mkdir -p repo1/repo_traffic_data
        
        # Generate CSV file list dynamically from centralized configuration
        csv_files=()
        while IFS='=' read -r prefix repo_name; do
          if [[ -n "$prefix" && -n "$repo_name" ]]; then
            csv_files+=("${prefix}_clones_history.csv")
            csv_files+=("${prefix}_visitors_history.csv")
          fi
        done <<< "$REPO_CONFIG"
        
        # Copy each file and check if it exists
        for file in "${csv_files[@]}"; do
          if [ -f "clones_monitor/$file" ]; then
            cp "clones_monitor/$file" "repo1/repo_traffic_data/$file"
          else
            echo "Warning: $file not found in clones_monitor directory"
          fi
        done
        
        cd repo1
        git config user.name 'GitHub Actions Bot'
        git config user.email '<>'
        
        if [[ -n $(git status -s repo_traffic_data) ]]; then
          git add -f repo_traffic_data/
          git commit -m "Update traffic statistics ($(TZ=Europe/Bucharest date +"%Y-%m-%d %H:%M:%S %Z"))"
          git pull --rebase origin main
          git push origin main
        fi